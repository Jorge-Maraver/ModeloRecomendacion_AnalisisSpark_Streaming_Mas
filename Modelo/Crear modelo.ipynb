{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb7f064",
   "metadata": {},
   "source": [
    "# Iteración 1: Modelo básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f067fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ALS-Reco\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2044fe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df loaded: 1000209\n",
      "ratings: 1000209\n",
      "train: 799983 test: 200226\n",
      "RMSE: 0.8627\n",
      "+------+-------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                            |\n",
      "+------+-------------------------------------------------------------------------------------------+\n",
      "|1     |[{572, 5.4786415}, {318, 4.6444197}, {3233, 4.636641}, {527, 4.6278057}, {953, 4.5249}]    |\n",
      "|3     |[{572, 5.399514}, {37, 4.7515473}, {811, 4.6856203}, {110, 4.5912414}, {318, 4.5528603}]   |\n",
      "|5     |[{3338, 4.3667197}, {1743, 4.30837}, {2309, 4.2805104}, {557, 4.1735425}, {771, 4.094903}] |\n",
      "|6     |[{572, 5.6090546}, {687, 5.005961}, {985, 4.822717}, {1164, 4.805526}, {2197, 4.75691}]    |\n",
      "|9     |[{572, 4.7859836}, {318, 4.5096383}, {1851, 4.474278}, {2905, 4.4189277}, {50, 4.398362}]  |\n",
      "|12    |[{572, 5.2123504}, {858, 4.692564}, {2309, 4.6253304}, {318, 4.5641427}, {2905, 4.5414977}]|\n",
      "|13    |[{572, 4.6868763}, {260, 4.186436}, {1198, 4.1248837}, {318, 4.1032095}, {2905, 4.098286}] |\n",
      "|15    |[{572, 4.8193903}, {37, 4.176042}, {318, 4.0676885}, {110, 4.0019608}, {3233, 3.9995651}]  |\n",
      "|16    |[{572, 4.990365}, {1780, 4.6377425}, {3092, 4.500677}, {2197, 4.4247456}, {3114, 4.41119}] |\n",
      "|17    |[{572, 5.2229166}, {787, 5.0604978}, {3338, 5.023401}, {989, 4.976643}, {1851, 4.9370685}] |\n",
      "+------+-------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "df = spark.read.parquet(\"/home/jovyan/work/datasets/df_ratings_full.parquet\")\n",
    "print(\"df loaded:\", df.count())\n",
    "\n",
    "ratings = (\n",
    "    df.select(\"userId\", \"filmId\", \"rating\")\n",
    "    .dropna()\n",
    "    .withColumn(\"userId\", F.col(\"userId\").cast(\"int\"))\n",
    "    .withColumn(\"filmId\", F.col(\"filmId\").cast(\"int\"))\n",
    "    .withColumn(\"rating\", F.col(\"rating\").cast(\"float\"))\n",
    "    .dropDuplicates([\"userId\", \"filmId\"])\n",
    ")\n",
    "print(\"ratings:\", ratings.count())\n",
    "\n",
    "train, test = ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"train:\", train.count(), \"test:\", test.count())\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"filmId\",\n",
    "    ratingCol=\"rating\",\n",
    "    rank=20,\n",
    "    maxIter=15,\n",
    "    regParam=0.1,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "model = als.fit(train)\n",
    "preds = model.transform(test)\n",
    "\n",
    "rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\").evaluate(preds)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "model.recommendForAllUsers(5).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a41ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n",
      "|userId|filmId|prediction|\n",
      "+------+------+----------+\n",
      "|1     |10    |3.3299048 |\n",
      "|1     |20    |2.7066665 |\n",
      "|2     |10    |3.3457944 |\n",
      "|3     |50    |4.482435  |\n",
      "+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "model_path = \"/home/jovyan/work/Modelo/Modelo_als/als1\"  # ajusta si cambiaste la ruta\n",
    "loaded = ALSModel.load(model_path)\n",
    "\n",
    "to_score = spark.createDataFrame(\n",
    "    [(1, 10), (1, 20), (2, 10), (3, 50)],\n",
    "    [\"userId\", \"filmId\"]\n",
    ")\n",
    "\n",
    "loaded.transform(to_score).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cba6c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab31e9",
   "metadata": {},
   "source": [
    "# Iteración 2: Modelo básico con aviso de porcentaje de entrenamiento estimado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afef005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ALS-Reco\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0314f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb2a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df loaded: 1000209\n",
      "ratings: 1000209\n",
      "train: 799983 test: 200226\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "df = spark.read.parquet(\"/home/jovyan/work/datasets/df_ratings_full.parquet\")\n",
    "print(\"df loaded:\", df.count())\n",
    "\n",
    "ratings = (\n",
    "    df.select(\"userId\", \"filmId\", \"rating\")\n",
    "    .dropna()\n",
    "    .withColumn(\"userId\", F.col(\"userId\").cast(\"int\"))\n",
    "    .withColumn(\"filmId\", F.col(\"filmId\").cast(\"int\"))\n",
    "    .withColumn(\"rating\", F.col(\"rating\").cast(\"float\"))\n",
    "    .dropDuplicates([\"userId\", \"filmId\"])\n",
    ")\n",
    "print(\"ratings:\", ratings.count())\n",
    "\n",
    "train, test = ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"train:\", train.count(), \"test:\", test.count())\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"filmId\",\n",
    "    ratingCol=\"rating\",\n",
    "    rank=20,\n",
    "    maxIter=15,\n",
    "    regParam=0.1,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a89020e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[job] 194 0.0% (0/2)\n",
      "[job] 196 0.0% (0/16)\n",
      "[job] 196 0.0% (0/16)\n",
      "[job] 196 6.2% (1/16)\n",
      "[job] 196 25.0% (4/16)\n",
      "[job] 197 12.5% (2/16)\n",
      "[job] 198 3.1% (10/318)\n",
      "[job] 198 7.2% (23/318)\n",
      "[job] 198 10.4% (33/318)\n",
      "[job] 198 12.6% (40/318)\n",
      "[job] 198 15.7% (50/318)\n",
      "[job] 198 18.9% (60/318)\n",
      "[job] 198 22.0% (70/318)\n",
      "[job] 198 25.2% (80/318)\n",
      "[job] 198 27.0% (86/318)\n",
      "[job] 198 29.9% (95/318)\n",
      "[job] 198 33.0% (105/318)\n",
      "[job] 198 36.8% (117/318)\n",
      "[job] 198 39.9% (127/318)\n",
      "[job] 198 42.8% (136/318)\n",
      "[job] 198 45.6% (145/318)\n",
      "[job] 198 48.4% (154/318)\n",
      "[job] 198 51.9% (165/318)\n",
      "[job] 198 55.0% (175/318)\n",
      "[job] 198 56.6% (180/318)\n",
      "[job] 198 59.7% (190/318)\n",
      "[job] 198 62.9% (200/318)\n",
      "[job] 198 66.4% (211/318)\n",
      "[job] 198 69.2% (220/318)\n",
      "[job] 198 72.3% (230/318)\n",
      "[job] 198 75.5% (240/318)\n",
      "[job] 198 78.6% (250/318)\n",
      "[job] 198 81.8% (260/318)\n",
      "[job] 198 84.9% (270/318)\n",
      "[job] 198 86.8% (276/318)\n",
      "[job] 198 89.9% (286/318)\n",
      "[job] 198 91.8% (292/318)\n",
      "[job] 198 95.0% (302/318)\n",
      "[job] 199 0.0% (0/308)\n"
     ]
    }
   ],
   "source": [
    "import threading, time, requests\n",
    "\n",
    "stop_flag = False\n",
    "\n",
    "def monitor_jobs(ui_port=4040, interval=0.5):\n",
    "    base = f\"http://localhost:{ui_port}/api/v1\"\n",
    "    try:\n",
    "        app_id = requests.get(f\"{base}/applications\").json()[0][\"id\"]\n",
    "    except Exception:\n",
    "        return\n",
    "    while not stop_flag:\n",
    "        try:\n",
    "            jobs = requests.get(f\"{base}/applications/{app_id}/jobs\").json()\n",
    "            active = [j for j in jobs if j.get(\"status\") == \"RUNNING\"]\n",
    "            if active:\n",
    "                j = active[0]\n",
    "                done = j.get(\"numCompletedTasks\", 0)\n",
    "                total = j.get(\"numTasks\", 0) or 1\n",
    "                pct = 100 * done / total\n",
    "                print(f\"[job] {j['jobId']} {pct:.1f}% ({done}/{total})\")\n",
    "            time.sleep(interval)\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "t = threading.Thread(target=monitor_jobs, daemon=True)\n",
    "t.start()\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "stop_flag = True\n",
    "t.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d68cfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Job 206] rdd at ALS.scala:727 -> 0.0% (0/2)\n",
      "[Job 207] isEmpty at ALS.scala:975 -> 0.0% (0/3)\n",
      "[Job 208] Pre-procesamiento (Bloques) -> 0.0% (0/16)\n",
      "[Job 208] Pre-procesamiento (Bloques) -> 6.2% (1/16)\n",
      "[Job 208] Pre-procesamiento (Bloques) -> 37.5% (6/16)\n",
      "[Job 209] Pre-procesamiento (Estadísticas) -> 12.5% (2/16)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 4.4% (14/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 6.9% (22/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 10.4% (33/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 12.6% (40/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 13.8% (44/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 16.7% (53/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 19.2% (61/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 22.3% (71/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 25.2% (80/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 28.3% (90/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 31.4% (100/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 34.6% (110/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 38.1% (121/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 40.9% (130/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 41.8% (133/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 45.6% (145/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 48.7% (155/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 50.9% (162/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 53.5% (170/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 55.3% (176/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 58.8% (187/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 61.6% (196/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 64.5% (205/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 67.0% (213/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 69.8% (222/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 73.3% (233/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 75.5% (240/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 78.6% (250/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 81.8% (260/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 84.0% (267/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 87.4% (278/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 90.3% (287/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 93.1% (296/318)\n",
      "[Job 210] Entrenando (Iteraciones Usuarios) -> 95.6% (304/318)\n",
      "[Job 211] Finalizando (Matriz Items) -> 0.6% (2/308)\n"
     ]
    }
   ],
   "source": [
    "import threading, time, requests\n",
    "\n",
    "stop_flag = False\n",
    "\n",
    "def monitor_jobs(ui_port=4040, interval=0.5):\n",
    "    base = f\"http://localhost:{ui_port}/api/v1\"\n",
    "    \n",
    "    # 1. Mapa de traducción: De \"Código Feo\" a \"Humano\"\n",
    "    # Estos números (1090, 1095) son las líneas de código de Spark donde ocurre la magia.\n",
    "    translations = {\n",
    "        \"ALS.scala:1090\": \"Entrenando (Iteraciones Usuarios)\",\n",
    "        \"ALS.scala:1095\": \"Finalizando (Matriz Items)\",\n",
    "        \"ALS.scala:988\":  \"Pre-procesamiento (Bloques)\",\n",
    "        \"ALS.scala:995\":  \"Pre-procesamiento (Estadísticas)\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        app_id = requests.get(f\"{base}/applications\").json()[0][\"id\"]\n",
    "    except Exception:\n",
    "        return\n",
    "\n",
    "    while not stop_flag:\n",
    "        try:\n",
    "            jobs = requests.get(f\"{base}/applications/{app_id}/jobs\").json()\n",
    "            # Filtramos solo los RUNNING\n",
    "            active = [j for j in jobs if j.get(\"status\") == \"RUNNING\"]\n",
    "            \n",
    "            if active:\n",
    "                j = active[0]\n",
    "                done = j.get(\"numCompletedTasks\", 0)\n",
    "                total = j.get(\"numTasks\", 0) or 1\n",
    "                pct = 100 * done / total\n",
    "                \n",
    "                # Obtenemos el nombre técnico original\n",
    "                raw_name = j.get(\"name\", \"\")\n",
    "                \n",
    "                # 2. Buscamos si el nombre técnico contiene alguna de nuestras claves\n",
    "                readable_name = \"Procesando...\" # Default\n",
    "                for key, val in translations.items():\n",
    "                    if key in raw_name:\n",
    "                        readable_name = val\n",
    "                        break\n",
    "                \n",
    "                # Si no encontramos traducción, usamos el nombre original recortado\n",
    "                if readable_name == \"Procesando...\":\n",
    "                     readable_name = raw_name[:30]\n",
    "\n",
    "                # Imprimimos bonito\n",
    "                # \\r al principio permite sobrescribir la línea (opcional, si te gusta efecto barra de carga)\n",
    "                print(f\"[Job {j['jobId']}] {readable_name} -> {pct:.1f}% ({done}/{total})\")\n",
    "            \n",
    "            time.sleep(interval)\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "t = threading.Thread(target=monitor_jobs, daemon=True)\n",
    "t.start()\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "stop_flag = True\n",
    "t.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4761cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8621\n"
     ]
    }
   ],
   "source": [
    "preds = model.transform(test)\n",
    "\n",
    "rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\").evaluate(preds)\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fef4bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee879fc",
   "metadata": {},
   "source": [
    "# Iteración 3: Modelo mejorado con mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dfbe92",
   "metadata": {},
   "source": [
    "## Búsqueda de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194a938d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ALS-Reco\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92e2c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df loaded: 1000209\n",
      "ratings: 1000209\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "df = spark.read.parquet(\"/home/jovyan/work/datasets/df_ratings_full.parquet\")\n",
    "print(\"df loaded:\", df.count())\n",
    "\n",
    "ratings = (\n",
    "    df.select(\"userId\", \"filmId\", \"rating\")\n",
    "    .dropna()\n",
    "    .withColumn(\"userId\", F.col(\"userId\").cast(\"int\"))\n",
    "    .withColumn(\"filmId\", F.col(\"filmId\").cast(\"int\"))\n",
    "    .withColumn(\"rating\", F.col(\"rating\").cast(\"float\"))\n",
    "    .dropDuplicates([\"userId\", \"filmId\"])\n",
    ")\n",
    "print(\"ratings:\", ratings.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc64eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: 20 0.1 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"filmId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(als.rank, [10, 20])\n",
    "             .addGrid(als.regParam, [0.05, 0.1])\n",
    "             .addGrid(als.maxIter, [10, 15])\n",
    "             .build())\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\"),\n",
    "    trainRatio=0.8, \n",
    "    parallelism=2    \n",
    ")\n",
    "\n",
    "model_tuned = tvs.fit(ratings)\n",
    "best_model = model_tuned.bestModel\n",
    "print(\"Mejores hiperparámetros:\", best_model.rank, best_model._java_obj.parent().getRegParam(), best_model._java_obj.parent().getMaxIter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad45de",
   "metadata": {},
   "source": [
    "Casualmente (o no tan casualmente) los hiperparámetros resultantes son los que ya usaba así que no tengo que reentrenar el modelo. Aún así, este es un ejemplo pequeño, lo suyo sería probar con un grid mucho mayor, más hiperparámetros y, sobre todo, más iteraciones, pero Codespace da para lo que da."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
